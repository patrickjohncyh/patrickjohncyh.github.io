---
layout: default
modal-id: 1
date: 2014-07-18
img: waldo.png
alt: image-alt
project-date: May-June 2019
client: IBM
client-link: https://www.ibm.com
demo: https://video.wixstatic.com/video/7fd7b8_a3a0baaba87d49e3b674c45f67c5b6f6/1080p/mp4/file.mp4
category: Deep Learning; Computer Vision; Gesture Recognition
description: WALDO is a Third Year Industrial Group Project done in collaboration with <strong>IBM</strong>. It is a <strong>deep learning</strong> enabled assisted living device meant to perform Makaton Sign recognition. I was part of the team that focused on the Machine Learning reasearch, development and deployment.</br></br>At the core of WALDO is it's deep neural network architecture which is based on a <strong><a href=https://arxiv.org/pdf/1412.0767.pdf>C3D</a></strong> network concatenated with an LSTM. This allows the model to learn the spatio-temporal features required for accurate gesture recognition.</br></br> Particulary challenging was having the model run on a <a href=https://developer.nvidia.com/embedded/jetson-nano-developer-kit>Jetson Nano</a> <strong>edge device</strong>. This required investigation into which parts of the model could be reduced, whilst achieving a good trade off between accuracy and performance.</br></br>For more information, do check out the <a href=https://github.com/patrickjohncyh/ibm-waldo>Github Repo</a> and also the team's weekly <a href=https://waldogroup13.wixsite.com/waldo>blog</a>.</br></br>Here is a short video demonstration,<video width="320" height="240" controls><source src="https://video.wixstatic.com/video/7fd7b8_a3a0baaba87d49e3b674c45f67c5b6f6/1080p/mp4/file.mp4" type="video/mp4"></video><!-- The network is fed with 30 Video Frames and outputs the probability that a given class is detected. -->
---
